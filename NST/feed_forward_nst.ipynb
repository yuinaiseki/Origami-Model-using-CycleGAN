{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config same to nst\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Hyperparameters\n",
    "IMG_SIZE = 128    #reduced to 128 for quick check(can use 256/512 later)       \n",
    "BATCH_SIZE = 6\n",
    "NUM_EPOCHS = 3\n",
    "LR = 1e-3\n",
    "CONTENT_WEIGHT = 1.0\n",
    "STYLE_WEIGHT   = 1e6   \n",
    "TV_WEIGHT      = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fielpaths\n",
    "CONTENT_ROOT = \"../Data/dataset/clean/animals_balanced\"   \n",
    "STYLE_ROOT= \"../Data/dataset/clean/origami_images\"\n",
    "SPLIT_ROOT   = \"../Data/dataset/split\"   \n",
    "CHECKPOINT_DIR = \"./checkpoints_nststyle\"\n",
    "SAMPLES_DIR    = \"./samples_nststyle\"\n",
    "\n",
    "for d in [SPLIT_ROOT, CHECKPOINT_DIR, SAMPLES_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "TARGET_CLASS = \"butterfly\" #single class (inital)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG Layer Configs and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vsame as NST\n",
    "LAYER_INDICES = {\n",
    "    'conv1_1': '0', \n",
    "    'conv1_2': '2', \n",
    "    'conv2_1': '5', \n",
    "    'conv2_2': '7',\n",
    "    'conv3_1': '10', \n",
    "    'conv3_2': '12', \n",
    "    'conv3_3': '14', \n",
    "    'conv3_4': '16',\n",
    "    'conv4_1': '19', \n",
    "    'conv4_2': '21', \n",
    "    'conv4_3': '23', \n",
    "    'conv4_4': '25',\n",
    "    'conv5_1': '28', \n",
    "    'conv5_2': '30', \n",
    "    'conv5_3': '32', \n",
    "    'conv5_4': '34'\n",
    "}\n",
    "\n",
    "LAYER_CONFIGS = {\n",
    "    'gatys': {\n",
    "        'content': ['conv4_2'],\n",
    "        'style': ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1'],\n",
    "        'style_weights': {\n",
    "            'conv1_1': 1.0,\n",
    "            'conv2_1': 0.8,\n",
    "            'conv3_1': 0.5,\n",
    "            'conv4_1': 0.3,\n",
    "            'conv5_1': 0.1\n",
    "        },\n",
    "    }\n",
    "}\n",
    "ACTIVE_LAYER_CONFIG = 'gatys'\n",
    "\n",
    "#normalization\n",
    "IMG_MEAN = [0.485, 0.456, 0.406]\n",
    "IMG_STD  = [0.229, 0.224, 0.225]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization for VGG and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exif_fix_and_open(path):\n",
    "    img = Image.open(path)\n",
    "    img = ImageOps.exif_transpose(img)\n",
    "    return img.convert(\"RGB\")\n",
    "\n",
    "#same as nst.py\n",
    "def normalize_for_vgg(x):\n",
    "    mean = torch.tensor(IMG_MEAN).view(1,3,1,1).to(DEVICE)\n",
    "    std  = torch.tensor(IMG_STD).view(1,3,1,1).to(DEVICE)\n",
    "    return (x - mean) / std\n",
    "\n",
    "def extract_features_batch(x, layers, model):\n",
    "    x_vgg = normalize_for_vgg(x)\n",
    "    cur = x_vgg\n",
    "    features = {}\n",
    "    layers_to_extract = {LAYER_INDICES[name]: name for name in layers}\n",
    "    for idx, layer in model._modules.items():\n",
    "        cur = layer(cur)\n",
    "        if idx in layers_to_extract:\n",
    "            features[layers_to_extract[idx]] = cur\n",
    "    return features\n",
    "\n",
    "def gram_matrix_batch(tensor):\n",
    "    b, c, h, w = tensor.size()\n",
    "    f = tensor.view(b, c, h*w)\n",
    "    return torch.bmm(f, f.transpose(1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train/val/test splits under ../Data/dataset/split/butterfly\n"
     ]
    }
   ],
   "source": [
    "def ensure_splits(class_name, val_frac=0.1, test_frac=0.05, seed=42):\n",
    "    random.seed(seed)\n",
    "    src_content = os.path.join(CONTENT_ROOT, class_name)\n",
    "    src_style   = os.path.join(STYLE_ROOT, class_name)\n",
    "    assert os.path.isdir(src_content), f\"Missing content folder: {src_content}\"\n",
    "    assert os.path.isdir(src_style), f\"Missing style folder: {src_style}\"\n",
    "\n",
    "    def split_list(files):\n",
    "        n = len(files)\n",
    "        n_val = int(n * val_frac)\n",
    "        n_test = int(n * test_frac)\n",
    "        return files[n_val+n_test:], files[:n_val], files[n_val:n_val+n_test]\n",
    "\n",
    "    def copy_split(src_folder, dst_folder, files):\n",
    "        os.makedirs(dst_folder, exist_ok=True)\n",
    "        for f in files:\n",
    "            src = os.path.join(src_folder, f)\n",
    "            dst = os.path.join(dst_folder, f)\n",
    "            if not os.path.exists(dst):\n",
    "                Image.open(src).convert(\"RGB\").save(dst, \"JPEG\", quality=90)\n",
    "\n",
    "    for domain in [\"content\", \"style\"]:\n",
    "        src = os.path.join(CONTENT_ROOT if domain==\"content\" else STYLE_ROOT, class_name)\n",
    "        files = [f for f in os.listdir(src) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
    "        random.shuffle(files)\n",
    "        train, val, test = split_list(files)\n",
    "        for split, flist in zip([\"train\",\"val\",\"test\"], [train,val,test]):\n",
    "            out_dir = os.path.join(SPLIT_ROOT, split, class_name)\n",
    "            copy_split(src, out_dir, flist)\n",
    "\n",
    "    print(f\"Created train/val/test splits under {SPLIT_ROOT}/{class_name}\")\n",
    "\n",
    "ensure_splits(TARGET_CLASS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "style_transform = content_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampler ready, number of train images: 1242\n"
     ]
    }
   ],
   "source": [
    "class SingleClassPairedSampler:\n",
    "        def __init__(self, split_root, class_name):\n",
    "            self.split_root = split_root\n",
    "            self.class_name = class_name\n",
    "            self._build_index()\n",
    "            self.split = 'train'\n",
    "\n",
    "        def _build_index(self):\n",
    "            for split in ['train','val','test']:\n",
    "                path = os.path.join(self.split_root, split, self.class_name)\n",
    "                files = sorted([\n",
    "                    os.path.join(path, f) for f in os.listdir(path)\n",
    "                    if f.lower().endswith(('.jpg','.jpeg','.png'))\n",
    "                ])\n",
    "                setattr(self, f\"{split}_files\", files)\n",
    "\n",
    "        def set_split(self, split): self.split = split\n",
    "\n",
    "        def sample_batch(self, batch_size):\n",
    "            files = getattr(self, f\"{self.split}_files\")\n",
    "            paths_c = random.choices(files, k=batch_size)\n",
    "            paths_s = random.choices(files, k=batch_size)\n",
    "            c_imgs = [content_transform(exif_fix_and_open(p)) for p in paths_c]\n",
    "            s_imgs = [style_transform(exif_fix_and_open(p)) for p in paths_s]\n",
    "            return torch.stack(c_imgs), torch.stack(s_imgs)\n",
    "\n",
    "sampler = SingleClassPairedSampler(SPLIT_ROOT, TARGET_CLASS)\n",
    "print(\"Sampler ready, number of train images:\", len(sampler.train_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Pre-Trained VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antranakhasi/Desktop/Projects/Origami model using CycleGAN/Origami-Model-using-CycleGAN/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/antranakhasi/Desktop/Projects/Origami model using CycleGAN/Origami-Model-using-CycleGAN/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#load pretrained VGG (same form nst.py)\n",
    "vgg = models.vgg19(pretrained=True).features.to(DEVICE).eval()\n",
    "for p in vgg.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module): #processes the image and extracts features while keeping style consistent\n",
    "    def __init__(self, in_c, out_c, kernel, stride):\n",
    "        super().__init__()\n",
    "        padding = kernel // 2\n",
    "        self.conv = nn.Conv2d(in_c, out_c, kernel, stride, padding)\n",
    "        self.inorm = nn.InstanceNorm2d(out_c, affine=True)\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.inorm(self.conv(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module): #learn style modifications\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.in1(self.conv1(x)))\n",
    "        out = self.in2(self.conv2(out))\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleConv(nn.Module): # upsampling the image (making it bigger)\n",
    "    def __init__(self, in_c, out_c, kernel, upsample=None):\n",
    "        super().__init__()\n",
    "        self.upsample = upsample\n",
    "        padding = kernel // 2\n",
    "        self.conv = nn.Conv2d(in_c, out_c, kernel, 1, padding)\n",
    "        self.inorm = nn.InstanceNorm2d(out_c, affine=True)\n",
    "    def forward(self, x):\n",
    "        if self.upsample:\n",
    "            x = F.interpolate(x, scale_factor=self.upsample, mode='nearest')\n",
    "        return F.relu(self.inorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvLayer(3, 32, 9, 1)\n",
    "        self.conv2 = ConvLayer(32, 64, 3, 2)\n",
    "        self.conv3 = ConvLayer(64, 128, 3, 2)\n",
    "        self.res1 = ResidualBlock(128)\n",
    "        self.res2 = ResidualBlock(128)\n",
    "        self.res3 = ResidualBlock(128)\n",
    "        self.res4 = ResidualBlock(128)\n",
    "        self.res5 = ResidualBlock(128)\n",
    "        self.up1 = UpsampleConv(128, 64, 3, upsample=2)\n",
    "        self.up2 = UpsampleConv(64, 32, 3, upsample=2)\n",
    "        self.conv_out = nn.Conv2d(32, 3, 9, 1, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.conv2(y)\n",
    "        y = self.conv3(y)\n",
    "        y = self.res1(y)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.up1(y)\n",
    "        y = self.up2(y)\n",
    "        y = self.conv_out(y)\n",
    "        return torch.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerNet().to(DEVICE)\n",
    "opt = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using steps_per_epoch = 80 (total_train_images=1242, batch_size=6)\n",
      "[Epoch 1 Step 0] Total 174016131915317248.0000\n",
      "[Epoch 1 Step 50] Total 257013316893802496.0000\n",
      "Epoch 1 done | Avg loss 233098887127944384.0000\n",
      "Using steps_per_epoch = 80 (total_train_images=1242, batch_size=6)\n",
      "[Epoch 2 Step 0] Total 132619175532167168.0000\n",
      "[Epoch 2 Step 50] Total 383216532840251392.0000\n",
      "Epoch 2 done | Avg loss 225436890313418336.0000\n",
      "Using steps_per_epoch = 80 (total_train_images=1242, batch_size=6)\n",
      "[Epoch 3 Step 0] Total 95563520552206336.0000\n",
      "[Epoch 3 Step 50] Total 218656269643284480.0000\n",
      "Epoch 3 done | Avg loss 196145614612017984.0000\n",
      "Using steps_per_epoch = 80 (total_train_images=1242, batch_size=6)\n",
      "[Epoch 4 Step 0] Total 134578762950901760.0000\n",
      "[Epoch 4 Step 50] Total 141924007430586368.0000\n",
      "Epoch 4 done | Avg loss 213499081971780800.0000\n",
      "Using steps_per_epoch = 80 (total_train_images=1242, batch_size=6)\n",
      "[Epoch 5 Step 0] Total 205968403674955776.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# content loss\u001b[39;00m\n\u001b[32m     35\u001b[39m c_feats = extract_features_batch(c_norm, content_layers, vgg)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m o_feats = \u001b[43mextract_features_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvgg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m c_loss = \u001b[38;5;28msum\u001b[39m(torch.mean((o_feats[l]-c_feats[l])**\u001b[32m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m content_layers)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# style loss\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mextract_features_batch\u001b[39m\u001b[34m(x, layers, model)\u001b[39m\n\u001b[32m     16\u001b[39m layers_to_extract = {LAYER_INDICES[name]: name \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m layers}\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, layer \u001b[38;5;129;01min\u001b[39;00m model._modules.items():\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     cur = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m layers_to_extract:\n\u001b[32m     20\u001b[39m         features[layers_to_extract[idx]] = cur\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/Origami model using CycleGAN/Origami-Model-using-CycleGAN/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/Origami model using CycleGAN/Origami-Model-using-CycleGAN/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/Origami model using CycleGAN/Origami-Model-using-CycleGAN/venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/Origami model using CycleGAN/Origami-Model-using-CycleGAN/venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "config = LAYER_CONFIGS[ACTIVE_LAYER_CONFIG]\n",
    "content_layers = config['content']\n",
    "style_layers = config['style']\n",
    "style_weights = config['style_weights']\n",
    "\n",
    "sampler.set_split('train')\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    \n",
    "    # steps_per_epoch = max(100, len(sampler.train_files)//BATCH_SIZE)  \n",
    "    \n",
    "    #for quick run\n",
    "    total_train_images = sampler.data['train']['content'].shape[0] if hasattr(sampler, 'data') else len(sampler.train_files)\n",
    "    min_steps = 20\n",
    "    max_steps = 80\n",
    "    estimated = max(1, total_train_images // BATCH_SIZE)\n",
    "    steps_per_epoch = min(max_steps, max(min_steps, estimated))\n",
    "    print(f\"Using steps_per_epoch = {steps_per_epoch} (total_train_images={total_train_images}, batch_size={BATCH_SIZE})\")\n",
    "    \n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        content_batch, style_batch = sampler.sample_batch(BATCH_SIZE)\n",
    "        content_batch, style_batch = content_batch.to(DEVICE), style_batch.to(DEVICE)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        output = model(content_batch)\n",
    "\n",
    "        c_norm, s_norm, o_norm = map(normalize_for_vgg, [content_batch, style_batch, output])\n",
    "\n",
    "        # content loss\n",
    "        c_feats = extract_features_batch(c_norm, content_layers, vgg)\n",
    "        o_feats = extract_features_batch(o_norm, content_layers, vgg)\n",
    "        c_loss = sum(torch.mean((o_feats[l]-c_feats[l])**2) for l in content_layers)\n",
    "\n",
    "        # style loss\n",
    "        s_feats = extract_features_batch(s_norm, style_layers, vgg)\n",
    "        s_grams = {l: gram_matrix_batch(s_feats[l]) for l in style_layers}\n",
    "        o_feats_style = extract_features_batch(o_norm, style_layers, vgg)\n",
    "        s_loss = 0\n",
    "        for l in style_layers:\n",
    "            Gs, Go = s_grams[l], gram_matrix_batch(o_feats_style[l])\n",
    "            w = style_weights.get(l, 1.0)\n",
    "            s_loss += w * torch.mean((Go - Gs)**2)\n",
    "\n",
    "        # TV loss\n",
    "        tv_loss = torch.mean(torch.abs(output[:, :, :, :-1] - output[:, :, :, 1:])) + \\\n",
    "                  torch.mean(torch.abs(output[:, :, :-1, :] - output[:, :, 1:, :]))\n",
    "\n",
    "        total_loss = CONTENT_WEIGHT*c_loss + STYLE_WEIGHT*s_loss + TV_WEIGHT*tv_loss\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "        global_step += 1\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"[Epoch {epoch} Step {step}] Total {total_loss.item():.4f}\")\n",
    "\n",
    "        if step % 300 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                sample_out = model(content_batch[:1]).cpu()\n",
    "                utils.save_image(sample_out, f\"{SAMPLES_DIR}/ep{epoch}_step{global_step}.png\")\n",
    "            model.train()\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{CHECKPOINT_DIR}/model_epoch{epoch}.pth\")\n",
    "    print(f\"Epoch {epoch} done | Avg loss {epoch_loss/steps_per_epoch:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerNet(\n",
       "  (conv1): ConvLayer(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "    (inorm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (conv2): ConvLayer(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (inorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (conv3): ConvLayer(\n",
       "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (inorm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (res1): ResidualBlock(\n",
       "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (in1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (in2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (res2): ResidualBlock(\n",
       "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (in1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (in2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (res3): ResidualBlock(\n",
       "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (in1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (in2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (res4): ResidualBlock(\n",
       "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (in1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (in2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (res5): ResidualBlock(\n",
       "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (in1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (in2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (up1): UpsampleConv(\n",
       "    (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (inorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (up2): UpsampleConv(\n",
       "    (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (inorm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (conv_out): Conv2d(32, 3, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerNet().to(DEVICE)\n",
    "checkpoint_path = f\"{CHECKPOINT_DIR}/model_epoch4.pth\"\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()  # set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test fior 1 image\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "img = Image.open(\"test_imgs/butterfly.jpg\").convert('RGB')\n",
    "\n",
    "content_tensor = content_transform(img).unsqueeze(0).to(DEVICE)  \n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(content_tensor)\n",
    "utils.save_image(output.cpu(), \"stylized_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
