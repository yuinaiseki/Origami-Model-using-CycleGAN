{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time, platform\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps USE_AMP: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/gpgs73x92nn02hjw6w7j3spm0000gn/T/ipykernel_84909/3228998659.py:5: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  elif getattr(torch, \"has_mps\", False) and torch.backends.mps.is_available():\n"
     ]
    }
   ],
   "source": [
    "#select computation device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    USE_AMP = True\n",
    "elif getattr(torch, \"has_mps\", False) and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    USE_AMP = False\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    USE_AMP = False\n",
    "\n",
    "print(\"Device:\", DEVICE, \"USE_AMP:\", USE_AMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "IMG_SIZE = 512       \n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 10\n",
    "LR = 1e-3 #changed\n",
    "CONTENT_WEIGHT = 0.5 #\n",
    "STYLE_WEIGHT   = 5e6   \n",
    "TV_WEIGHT      = 0 #for smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fielpaths\n",
    "CONTENT_ROOT = \"../Data/dataset/clean/animals_balanced\"   \n",
    "STYLE_ROOT= \"../Data/dataset/clean/origami_images\"\n",
    "SPLIT_ROOT   = \"../Data/dataset/split\"   \n",
    "CHECKPOINT_DIR = \"./checkpoints_nststyle\"\n",
    "SAMPLES_DIR    = \"./samples_nststyle\"\n",
    "\n",
    "for d in [SPLIT_ROOT, CHECKPOINT_DIR, SAMPLES_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    \n",
    "TARGET_CLASS = \"butterfly\" #single class (inital)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for root in ['content', 'style']:\n",
    "        path = os.path.join(SPLIT_ROOT, root, split, TARGET_CLASS)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_MEAN = [0.485, 0.456, 0.406]\n",
    "IMG_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "def list_images(dir_):\n",
    "    return sorted([os.path.join(dir_,f) for f in os.listdir(dir_)\n",
    "                   if f.lower().endswith(('.jpg','.jpeg','.png'))])\n",
    "    \n",
    "def exif_fix_and_open(path):\n",
    "    img = Image.open(path)\n",
    "    img = ImageOps.exif_transpose(img)\n",
    "    return img.convert(\"RGB\")\n",
    "\n",
    "def simple_bar(step, total, epoch, loss=None, bar_len=20):\n",
    "    pct = step / total\n",
    "    filled = int(bar_len * pct)\n",
    "    bar = \"=\" * filled + \".\" * (bar_len - filled)\n",
    "    msg = f\"\\rEpoch {epoch}: {bar} {pct*100:5.1f}%\"\n",
    "    if loss is not None:\n",
    "        msg += f\" | loss: {loss:.4f}\"\n",
    "    print(msg, end=\"\", flush=True)\n",
    "    if step == total:\n",
    "        print()  # newline at end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as nst.py\n",
    "def normalize_for_vgg(x):\n",
    "    mean = torch.tensor(IMG_MEAN).view(1,3,1,1).to(DEVICE)\n",
    "    std  = torch.tensor(IMG_STD).view(1,3,1,1).to(DEVICE)\n",
    "    return (x - mean) / std\n",
    "\n",
    "# def extract_features_batch(x, layers, model):\n",
    "#     x_vgg = normalize_for_vgg(x)\n",
    "#     cur = x_vgg\n",
    "#     features = {}\n",
    "#     layers_to_extract = {LAYER_INDICES[name]: name for name in layers}\n",
    "#     for idx, layer in model._modules.items():\n",
    "#         cur = layer(cur)\n",
    "#         if idx in layers_to_extract:\n",
    "#             features[layers_to_extract[idx]] = cur\n",
    "#     return features\n",
    "\n",
    "def gram_matrix_batch(tensor):\n",
    "    b, c, h, w = tensor.size()\n",
    "    f = tensor.view(b, c, h*w)\n",
    "    return torch.bmm(f, f.transpose(1,2)) / (c * h * w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splits ready under ../Data/dataset/split/butterfly\n"
     ]
    }
   ],
   "source": [
    "def ensure_splits(class_name, val_frac=0.1, test_frac=0.05, seed=42):\n",
    "    random.seed(seed)\n",
    "    for domain, root in [(\"content\", CONTENT_ROOT), (\"style\", STYLE_ROOT)]:\n",
    "        src = os.path.join(root, class_name)\n",
    "        assert os.path.isdir(src), f\"Missing {domain} folder: {src}\"\n",
    "        files = [f for f in os.listdir(src) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
    "        random.shuffle(files)\n",
    "        n = len(files); n_val=int(n*val_frac); n_test=int(n*test_frac)\n",
    "        splits = {\n",
    "            \"train\": files[n_val+n_test:],\n",
    "            \"val\":   files[:n_val],\n",
    "            \"test\":  files[n_val:n_val+n_test]\n",
    "        }\n",
    "        for split, names in splits.items():\n",
    "            out = os.path.join(SPLIT_ROOT, domain, split, class_name)\n",
    "            os.makedirs(out, exist_ok=True)\n",
    "            for f in names:\n",
    "                srcf = os.path.join(src, f); dstf = os.path.join(out, f)\n",
    "                if not os.path.exists(dstf):\n",
    "                    exif_fix_and_open(srcf).save(dstf, \"JPEG\", quality=90)\n",
    "    print(f\"splits ready under {SPLIT_ROOT}/{class_name}\")\n",
    "\n",
    "ensure_splits(TARGET_CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG Layer Configs and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Vsame as NST\n",
    "# LAYER_INDICES = {\n",
    "#     'conv1_1': '0', \n",
    "#     'conv1_2': '2', \n",
    "#     'conv2_1': '5', \n",
    "#     'conv2_2': '7',\n",
    "#     'conv3_1': '10', \n",
    "#     'conv3_2': '12', \n",
    "#     'conv3_3': '14', \n",
    "#     'conv3_4': '16',\n",
    "#     'conv4_1': '19', \n",
    "#     'conv4_2': '21', \n",
    "#     'conv4_3': '23', \n",
    "#     'conv4_4': '25',\n",
    "#     'conv5_1': '28', \n",
    "#     'conv5_2': '30', \n",
    "#     'conv5_3': '32', \n",
    "#     'conv5_4': '34'\n",
    "# }\n",
    "\n",
    "# LAYER_CONFIGS = {\n",
    "#     'gatys': {\n",
    "#         'content': ['conv4_2'],\n",
    "#         'style': ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1'],\n",
    "#         'style_weights': {\n",
    "#             'conv1_1': 1.0,\n",
    "#             'conv2_1': 0.8,\n",
    "#             'conv3_1': 0.5,\n",
    "#             'conv4_1': 0.3,\n",
    "#             'conv5_1': 0.1\n",
    "#         },\n",
    "#     }\n",
    "# }\n",
    "# ACTIVE_LAYER_CONFIG = 'gatys'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization for VGG and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = transforms.Compose([transforms.Resize(IMG_SIZE),\n",
    "                               transforms.CenterCrop(IMG_SIZE),\n",
    "                               transforms.ToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_train_dir = os.path.join(SPLIT_ROOT, \"content\", \"train\", TARGET_CLASS)\n",
    "style_train_dir   = os.path.join(SPLIT_ROOT, \"style\",   \"train\", TARGET_CLASS)\n",
    "CONTENT_FILES = list_images(content_train_dir)\n",
    "STYLE_FILES   = list_images(style_train_dir)\n",
    "assert len(CONTENT_FILES)>0 and len(STYLE_FILES)>0, \"need images to train\"\n",
    "\n",
    "def sample_content_batch(batch_size):\n",
    "    paths = random.choices(CONTENT_FILES, k=batch_size)\n",
    "    tensors = [train_tf(exif_fix_and_open(p)) for p in paths]\n",
    "    return torch.stack(tensors).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Pre-Trained VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antranakhasi/Desktop/Projects/Origami model using CycleGAN/Origami-Model-using-CycleGAN/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/antranakhasi/Desktop/Projects/Origami model using CycleGAN/Origami-Model-using-CycleGAN/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "LAYER_INDICES = {\n",
    "    'conv1_1': 0,  'conv1_2': 2,\n",
    "    'conv2_1': 5,  'conv2_2': 7,\n",
    "    'conv3_1': 10, 'conv3_2': 12, 'conv3_3': 14, 'conv3_4': 16,\n",
    "    'conv4_1': 19, 'conv4_2': 21, 'conv4_3': 23, 'conv4_4': 25,\n",
    "    'conv5_1': 28, 'conv5_2': 30, 'conv5_3': 32, 'conv5_4': 34\n",
    "}\n",
    "\n",
    "CONTENT_LAYERS = ['conv4_2']\n",
    "STYLE_LAYERS   = ['conv1_1','conv2_1','conv3_1','conv4_1','conv5_1','conv4_2']\n",
    "STYLE_LAYER_WEIGHTS = {\n",
    "    'conv1_1': 1.0,\n",
    "    'conv2_1': 0.8,\n",
    "    'conv3_1': 0.6,\n",
    "    'conv4_1': 0.4,\n",
    "    'conv5_1': 0.2,\n",
    "    'conv4_2': 0.2,   # adds structural push\n",
    "}\n",
    "vgg = models.vgg19(pretrained=True).features.to(DEVICE).eval()\n",
    "for p in vgg.parameters(): p.requires_grad = False\n",
    "\n",
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self, vgg, layer_indices):\n",
    "        super().__init__()\n",
    "        self.vgg = vgg\n",
    "        self.idx_to_name = {idx: name for name, idx in layer_indices.items()}\n",
    "        self.watch = set(layer_indices.values())\n",
    "    def forward(self, x):\n",
    "        feats = {}\n",
    "        cur = x\n",
    "        for i, layer in self.vgg._modules.items():\n",
    "            i = int(i)\n",
    "            cur = layer(cur)\n",
    "            if i in self.watch:\n",
    "                feats[self.idx_to_name[i]] = cur\n",
    "        return feats\n",
    "\n",
    "vgg_feat = VGGFeatureExtractor(vgg, LAYER_INDICES).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_c, out_c, k, s):\n",
    "        super().__init__()\n",
    "        self.pad = nn.ReflectionPad2d(k//2)\n",
    "        self.conv = nn.Conv2d(in_c, out_c, k, s, 0)\n",
    "        self.inorm = nn.InstanceNorm2d(out_c, affine=False)\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.inorm(self.conv(self.pad(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResidualBlock(nn.Module): #learn style modifications\n",
    "#     def __init__(self, channels):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "#         self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
    "#         self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "#         self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.in1(self.conv1(x)))\n",
    "#         out = self.in2(self.conv2(out))\n",
    "#         return out + x\n",
    "\n",
    "# class StylizedResidualBlock(nn.Module):\n",
    "#     def __init__(self, channels):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "#         self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
    "#         self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "#         self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
    "        \n",
    "#         # style gate to enhance stylized contrast/edges\n",
    "#         self.style_gate = nn.Sequential(\n",
    "#             nn.Conv2d(channels, channels, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.in1(self.conv1(x)))\n",
    "#         out = self.in2(self.conv2(out))\n",
    "#         gate = self.style_gate(out)\n",
    "#         # modulate residual with learned style gate\n",
    "#         out = out * gate + x\n",
    "        \n",
    "#         return out\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(c, c, 3, 1, 0),\n",
    "            nn.InstanceNorm2d(c, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(c, c, 3, 1, 0),\n",
    "            nn.InstanceNorm2d(c, affine=False),\n",
    "        )\n",
    "    def forward(self, x): return x + self.block(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UpsampleConv(nn.Module): # upsampling the image (making it bigger)\n",
    "#     def __init__(self, in_c, out_c, kernel, upsample=None):\n",
    "#         super().__init__()\n",
    "#         self.upsample = upsample\n",
    "#         padding = kernel // 2\n",
    "#         self.conv = nn.Conv2d(in_c, out_c, kernel, 1, padding)\n",
    "#         self.inorm = nn.InstanceNorm2d(out_c, affine=True)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         if self.upsample:\n",
    "#             x = F.interpolate(x, scale_factor=self.upsample, mode='nearest')\n",
    "            \n",
    "#         return F.relu(self.inorm(self.conv(x)))\n",
    "class UpNearestConv(nn.Module):\n",
    "    def __init__(self, in_c, out_c, scale=2, k=3):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.conv = nn.Conv2d(in_c, out_c, k, 1, padding=k//2)\n",
    "        self.inorm = nn.InstanceNorm2d(out_c, affine=False)\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=self.scale, mode='nearest')\n",
    "        x = self.conv(x)\n",
    "        return F.relu(self.inorm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerNet(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = ConvLayer(3, 32, 9, 1)\n",
    "#         self.conv2 = ConvLayer(32, 64, 3, 2)\n",
    "#         self.conv3 = ConvLayer(64, 128, 3, 2)\n",
    "#         self.res1 = ResidualBlock(128)\n",
    "#         self.res2 = ResidualBlock(128)\n",
    "#         self.res3 = ResidualBlock(128)\n",
    "#         self.res4 = ResidualBlock(128)\n",
    "#         self.res5 = ResidualBlock(128)\n",
    "#         self.up1 = UpsampleConv(128, 64, 3, upsample=2)\n",
    "#         self.up2 = UpsampleConv(64, 32, 3, upsample=2)\n",
    "#         self.conv_out = nn.Conv2d(32, 3, 9, 1, 4)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         y = self.conv1(x)\n",
    "#         y = self.conv2(y)\n",
    "#         y = self.conv3(y)\n",
    "#         y = self.res1(y)\n",
    "#         y = self.res2(y)\n",
    "#         y = self.res3(y)\n",
    "#         y = self.res4(y)\n",
    "#         y = self.res5(y)\n",
    "#         y = self.up1(y)\n",
    "#         y = self.up2(y)\n",
    "#         y = self.conv_out(y)\n",
    "#         return torch.sigmoid(y)\n",
    "\n",
    "# class TransformerNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = ConvLayer(3, 32, 9, 1)\n",
    "#         self.conv2 = ConvLayer(32, 64, 3, 2)\n",
    "#         self.conv3 = ConvLayer(64, 128, 3, 2)\n",
    "\n",
    "#         #stylized Residuals\n",
    "#         self.res1 = StylizedResidualBlock(128)\n",
    "#         self.res2 = StylizedResidualBlock(128)\n",
    "#         self.res3 = StylizedResidualBlock(128)\n",
    "#         self.res4 = StylizedResidualBlock(128)\n",
    "#         self.res5 = StylizedResidualBlock(128)\n",
    "\n",
    "#         self.up1 = UpsampleConv(128, 64, 3, upsample=2)\n",
    "#         self.up2 = UpsampleConv(64, 32, 3, upsample=2)\n",
    "#         self.conv_out = nn.Conv2d(32, 3, 9, 1, 4)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         y = self.conv1(x)\n",
    "#         y = self.conv2(y)\n",
    "#         y = self.conv3(y)\n",
    "#         y = self.res1(y)\n",
    "#         y = self.res2(y)\n",
    "#         y = self.res3(y)\n",
    "#         y = self.res4(y)\n",
    "#         y = self.res5(y)\n",
    "#         y = self.up1(y)\n",
    "#         y = self.up2(y)\n",
    "#         y = self.conv_out(y)\n",
    "#         return torch.sigmoid(y)\n",
    "\n",
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c1 = ConvLayer(3,   32, 9, 1)\n",
    "        self.c2 = ConvLayer(32,  64, 3, 2)\n",
    "        self.c3 = ConvLayer(64, 128, 3, 2)\n",
    "        self.r1 = ResidualBlock(128); self.r2 = ResidualBlock(128); self.r3 = ResidualBlock(128)\n",
    "        self.r4 = ResidualBlock(128); self.r5 = ResidualBlock(128)\n",
    "        self.u1 = UpNearestConv(128, 64)\n",
    "        self.u2 = UpNearestConv(64,  32)\n",
    "\n",
    "        self.pad_out = nn.ReflectionPad2d(4)\n",
    "        self.conv_out = nn.Conv2d(32, 3, 9, 1, 0)\n",
    "    def forward(self, x):\n",
    "        y = self.c1(x); y = self.c2(y); y = self.c3(y)\n",
    "        y = self.r1(y); y = self.r2(y); y = self.r3(y); y = self.r4(y); y = self.r5(y)\n",
    "        y = self.u1(y); y = self.u2(y)\n",
    "        y = self.pad_out(y); y = self.conv_out(y)\n",
    "        return torch.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerNet().to(DEVICE)\n",
    "opt = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "def tv_loss_fn(x):\n",
    "    return torch.mean(torch.abs(x[:,:,:,1:] - x[:,:,:,:-1])) + \\\n",
    "           torch.mean(torch.abs(x[:,:,1:,:] - x[:,:,:-1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 16\u001b[39m\n",
      "\u001b[32m     14\u001b[39m style_dir = os.path.join(STYLE_ROOT, TARGET_CLASS)\n",
      "\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# recompute grams at 512\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m style_grams = \u001b[43mprecompute_style_grams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     18\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCenterCrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     19\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\n",
      "\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mprecompute_style_grams\u001b[39m\u001b[34m(style_dir, transform, chunk)\u001b[39m\n",
      "\u001b[32m      8\u001b[39m         feats = vgg_feat(normalize_for_vgg(batch))\n",
      "\u001b[32m      9\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch.size(\u001b[32m0\u001b[39m)):\n",
      "\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m             grams.append({l: \u001b[43mgram_matrix_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m:\u001b[49m\u001b[43mj\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m feats.keys()})\n",
      "\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrecomputed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(grams)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m style gram dicts.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m grams\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def precompute_style_grams(style_dir, transform, chunk=8):\n",
    "    files = list_images(style_dir)\n",
    "    grams = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(files), chunk):\n",
    "            paths = files[i:i+chunk]\n",
    "            batch = torch.stack([transform(exif_fix_and_open(p)) for p in paths]).to(DEVICE)\n",
    "            feats = vgg_feat(normalize_for_vgg(batch))\n",
    "            for j in range(batch.size(0)):\n",
    "                grams.append({l: gram_matrix_batch(feats[l][j:j+1]).cpu() for l in feats.keys()})\n",
    "    print(f\"Precomputed {len(grams)} style gram dicts.\")\n",
    "    return grams\n",
    "\n",
    "style_dir = os.path.join(STYLE_ROOT, TARGET_CLASS)\n",
    "# recompute grams at 512\n",
    "style_grams = precompute_style_grams(style_dir, \n",
    "    transforms.Compose([transforms.Resize(IMG_SIZE),\n",
    "                        transforms.CenterCrop(IMG_SIZE),\n",
    "                        transforms.ToTensor()]),\n",
    "    chunk=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (no dataloader)…\n",
      "Epoch 1: ====................  20.0% | loss: 24.14405E1 S200/1000 | loss 26.4994\n",
      "Epoch 1: ========............  40.0% | loss: 18.2296E1 S400/1000 | loss 21.0941\n",
      "Epoch 1: ============........  60.0% | loss: 14.4558E1 S600/1000 | loss 19.6946\n",
      "Epoch 1: ================....  80.0% | loss: 13.2063E1 S800/1000 | loss 17.7973\n",
      "Epoch 1: ==================== 100.0% | loss: 11.0591\n",
      "E1 S1000/1000 | loss 15.2456\n",
      "[E1] checkpoint saved.\n",
      "Epoch 2: ====................  20.0% | loss: 18.7341E2 S200/1000 | loss 14.3131\n",
      "Epoch 2: ========............  40.0% | loss: 8.60605E2 S400/1000 | loss 13.7263\n",
      "Epoch 2: ============........  60.0% | loss: 6.91944E2 S600/1000 | loss 13.0267\n",
      "Epoch 2: ================....  80.0% | loss: 7.74632E2 S800/1000 | loss 12.0806\n",
      "Epoch 2: ==================== 100.0% | loss: 7.44248\n",
      "E2 S1000/1000 | loss 11.1061\n",
      "[E2] checkpoint saved.\n",
      "Epoch 3: ====................  20.0% | loss: 10.1141E3 S200/1000 | loss 11.5658\n",
      "Epoch 3: ========............  40.0% | loss: 5.91340E3 S400/1000 | loss 10.8631\n",
      "Epoch 3: ============........  60.0% | loss: 9.52651E3 S600/1000 | loss 11.7877\n",
      "Epoch 3: ================....  80.0% | loss: 27.6707E3 S800/1000 | loss 12.1525\n",
      "Epoch 3: ==================== 100.0% | loss: 25.8874\n",
      "E3 S1000/1000 | loss 10.4699\n",
      "[E3] checkpoint saved.\n",
      "Epoch 4: ====................  20.0% | loss: 5.40036E4 S200/1000 | loss 11.5797\n",
      "Epoch 4: ========............  40.0% | loss: 24.5843E4 S400/1000 | loss 10.7392\n",
      "Epoch 4: ============........  60.0% | loss: 8.82764E4 S600/1000 | loss 11.8632\n",
      "Epoch 4: ================....  80.0% | loss: 6.40610E4 S800/1000 | loss 12.7539\n",
      "Epoch 4: ==================== 100.0% | loss: 4.59096\n",
      "E4 S1000/1000 | loss 10.4822\n",
      "[E4] checkpoint saved.\n",
      "Epoch 5: ====................  20.0% | loss: 27.7538E5 S200/1000 | loss 10.9001\n",
      "Epoch 5: ========............  40.0% | loss: 48.8085E5 S400/1000 | loss 11.3412\n",
      "Epoch 5: ============........  60.0% | loss: 4.08462E5 S600/1000 | loss 10.5248\n",
      "Epoch 5: ================....  80.0% | loss: 4.33404E5 S800/1000 | loss 9.8526\n",
      "Epoch 5: ==================== 100.0% | loss: 8.93232\n",
      "E5 S1000/1000 | loss 9.4419\n",
      "[E5] checkpoint saved.\n",
      "Epoch 6: ====................  20.0% | loss: 11.0864E6 S200/1000 | loss 11.3464\n",
      "Epoch 6: ========............  40.0% | loss: 8.36847E6 S400/1000 | loss 10.2836\n",
      "Epoch 6: ============........  60.0% | loss: 13.6542E6 S600/1000 | loss 10.6310\n",
      "Epoch 6: ================....  80.0% | loss: 7.86155E6 S800/1000 | loss 11.6776\n",
      "Epoch 6: ==================== 100.0% | loss: 4.07490\n",
      "E6 S1000/1000 | loss 10.6915\n",
      "[E6] checkpoint saved.\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler() if (USE_AMP and DEVICE.type==\"cuda\") else None\n",
    "steps_per_epoch = max(1000, len(CONTENT_FILES)//BATCH_SIZE)\n",
    "\n",
    "print(\"Training (no dataloader)…\")\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for step in range(1, steps_per_epoch+1):\n",
    "        content_batch = sample_content_batch(BATCH_SIZE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = model(content_batch)\n",
    "                c_norm, o_norm = normalize_for_vgg(content_batch), normalize_for_vgg(out)\n",
    "                c_feats, o_feats = vgg_feat(c_norm), vgg_feat(o_norm)\n",
    "                c_loss = torch.mean((o_feats[CONTENT_LAYERS[0]] - c_feats[CONTENT_LAYERS[0]])**2)\n",
    "                Gs = random.choice(style_grams); s_loss = 0.0\n",
    "                for l in STYLE_LAYERS:\n",
    "                    Go = gram_matrix_batch(o_feats[l])\n",
    "                    s_loss += STYLE_LAYER_WEIGHTS[l]*torch.mean((Go - Gs[l].to(DEVICE))**2)\n",
    "                tv = tv_loss_fn(out)\n",
    "                total = CONTENT_WEIGHT*c_loss + STYLE_WEIGHT*s_loss + TV_WEIGHT*tv\n",
    "            scaler.scale(total).backward(); scaler.step(opt); scaler.update()\n",
    "        else:\n",
    "            out = model(content_batch)\n",
    "            c_norm, o_norm = normalize_for_vgg(content_batch), normalize_for_vgg(out)\n",
    "            c_feats, o_feats = vgg_feat(c_norm), vgg_feat(o_norm)\n",
    "            c_loss = torch.mean((o_feats[CONTENT_LAYERS[0]] - c_feats[CONTENT_LAYERS[0]])**2)\n",
    "            Gs = random.choice(style_grams); s_loss = 0.0\n",
    "            for l in STYLE_LAYERS:\n",
    "                Go = gram_matrix_batch(o_feats[l])\n",
    "                s_loss += STYLE_LAYER_WEIGHTS[l]*torch.mean((Go - Gs[l].to(DEVICE))**2)\n",
    "            tv = tv_loss_fn(out)\n",
    "            total = CONTENT_WEIGHT*c_loss + STYLE_WEIGHT*s_loss + TV_WEIGHT*tv\n",
    "            total.backward(); opt.step()\n",
    "\n",
    "        running += total.item()\n",
    "        \n",
    "        simple_bar(step, steps_per_epoch, epoch, loss=total.item())\n",
    "         \n",
    "        if step % 200 == 0:\n",
    "            avg = running/200; running = 0.0\n",
    "            print(f\"E{epoch} S{step}/{steps_per_epoch} | loss {avg:.4f}\")\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                utils.save_image(out[:1].cpu(), f\"{SAMPLES_DIR}/ep{epoch}_st{step}.png\")\n",
    "            model.train()\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"johnson_nold_epoch{epoch}.pth\"))\n",
    "    print(f\"[E{epoch}] checkpoint saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing for 1 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved stylized image to ./samples_nststyle/stylized_image.png\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_img_path = \"test_imgs/butterfly.jpg\"\n",
    "if os.path.exists(test_img_path):\n",
    "    x = train_tf(exif_fix_and_open(test_img_path)).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad(): y = model(x).cpu()\n",
    "    utils.save_image(y, os.path.join(SAMPLES_DIR, \"stylized_image.png\"))\n",
    "    print(\"Saved stylized image to\", os.path.join(SAMPLES_DIR, \"stylized_image.png\"))\n",
    "else:\n",
    "    print(\"Test image not found at\", test_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
